Requirements Notes:
Verification of 1.1.3/4/5: This is very dependent on what we define as the environment / target. Can we be more specific about the nature of the detection? Could include size, color, standoff distance, lighting environment, reflectivity, angle of incidence? What kind of 'similar' targets do we include that should or should not count as targets? Is there only one plausible target in the scene at a time? If not, how many parallel tracks do we support?

Level 3, 1.2.1: Clarify that we want to avoid obstacles of both a known and unknown nature. Maybe split this one into two verifications, one for each path? Solution can then include online mapping and localization as a means of obstacle avoidance.

Level 3, 1.2.2: Might be worth noting the local distance between waypoints. Position includes full pose (atttude/heading + xyz)? Verify through simulation and external tracking (Vicon). If we have the time, we can run the trajectory against a position controller with a vehicle model and report the ability of the planner to take into account vehicle dynamics (e.g. no point turns for a non-holonomic ground vehicle)

Level 3, 1.2.4: Same as 1.2.1, split between a priori obstacles and unknown obstacles

1.3.1: Specify the vehicle configuration / processing configuration for 'search'. Can we measure this by hovering in place with the camera(s) running at full CPU burn?

2.1.1: Pose estimate should be specified for both angular and translational DOF. +- 10 degrees? Verification through static test.

2.1: Specify whether the targets are dynamic or static

2.1.1: Are the targets visually distinctive / unique?

2.1.3: Is this absolute position or relative position? Presumably the multi-vehicle collaboration portion can make use of shared information like this, in which case the track must be shared in absolute coordinates

2.2: Is that target speed or our vehicle speed?

3.1.1: Propose changing to autonomously hover within specified bounding box on location and heading

3.1.3: Define fatal collision as a max impact force (2g?) and off-vertical pose limits

4.4: Maybe change this to 'targeting or navigation commands'? We're certainly going to tell it to start and stop among other things.

5.2.1: Is the delivery zone given or determined by proximity to target? 

5.3.1: Verify by static test

5.3.2: Verify performance by measurement of delivered article relative to target.

6.1: Specify system configuration (similar to 1.3.1)

7.1: Do we want to get into bandwidth limits / minima?

8.3.1: A lot of the POD is based on the target's relative size and pose to the camera system

9: System shall not interfere with Vicon or suffer fault due to Vicon operation

9.2.3: This may be a problem. Xtion depth map works in IR in the dark (just tested it), so state estimate will probably be OK (needs testing). However, target tracking is done via RGB data - which does not work in the dark. The Xtion uses structured IR light under the hood, which means the IR image is speckled and of relatively poor quality for doing visual work. Possible options might be an IR emitter on the target for nighttime ops?





